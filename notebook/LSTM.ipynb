{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037fbfd3",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7d7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfdd3d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       item_id store_id  wm_yr_wk  sales  sell_price  month  year\n",
      "0  FOODS_1_001     CA_1     11101     10         2.0      1  2011\n",
      "1  FOODS_1_001     CA_1     11101     10         2.0      2  2011\n",
      "2  FOODS_1_001     CA_1     11102      6         2.0      2  2011\n",
      "3  FOODS_1_001     CA_1     11103     10         2.0      2  2011\n",
      "4  FOODS_1_001     CA_1     11104     13         2.0      2  2011\n"
     ]
    }
   ],
   "source": [
    "weekly_data = pd.read_csv(r'C:\\Users\\wwwsu\\Desktop\\All folders\\Logistics_demand\\data\\processed\\weekly_aggregated_sales.csv')\n",
    "print(weekly_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aaff29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10183660 entries, 0 to 10183659\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Dtype  \n",
      "---  ------      -----  \n",
      " 0   item_id     object \n",
      " 1   store_id    object \n",
      " 2   wm_yr_wk    int64  \n",
      " 3   sales       int64  \n",
      " 4   sell_price  float64\n",
      " 5   month       int64  \n",
      " 6   year        int64  \n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 543.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(weekly_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d4e81",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a9eb5",
   "metadata": {},
   "source": [
    "LSTM cannot directly handle strings, so we convert item_id and store_id to integer indices. Later, the model will use embeddings for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3555ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 3049\n",
      "Number of unique stores: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create categorical mappings\n",
    "weekly_data['item_id_cat'] = weekly_data['item_id'].astype('category').cat.codes\n",
    "weekly_data['store_id_cat'] = weekly_data['store_id'].astype('category').cat.codes\n",
    "\n",
    "# Save mappings if needed for inference\n",
    "item_id_mapping = dict(enumerate(weekly_data['item_id'].astype('category').cat.categories))\n",
    "store_id_mapping = dict(enumerate(weekly_data['store_id'].astype('category').cat.categories))\n",
    "\n",
    "print(f\"Number of unique items: {weekly_data['item_id_cat'].nunique()}\")\n",
    "print(f\"Number of unique stores: {weekly_data['store_id_cat'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1e107",
   "metadata": {},
   "source": [
    "### Feature Engineering and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310fee7",
   "metadata": {},
   "source": [
    "We will use the following features:\n",
    "\n",
    "Numeric: sales, sell_price, month, year\n",
    "\n",
    "Categorical (encoded): item_id_cat, store_id_cat\n",
    "\n",
    "Scale numeric features using MinMaxScaler to normalize values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "235dc8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/62/27/585859e72e117fe861c2079bcba35591a84f801e21bc1ab85bce6ce60305/scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\wwwsu\\desktop\\all folders\\logistics_demand\\venv\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\wwwsu\\desktop\\all folders\\logistics_demand\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/da/d3/13ee227a148af1c693654932b8b0b02ed64af5e1f7406d56b088b57574cd/joblib-1.5.0-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/307.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/307.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/307.7 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/307.7 kB 217.9 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 41.0/307.7 kB 196.9 kB/s eta 0:00:02\n",
      "   ------- ------------------------------- 61.4/307.7 kB 252.2 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 81.9/307.7 kB 305.0 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 92.2/307.7 kB 308.0 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 122.9/307.7 kB 327.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 174.1/307.7 kB 436.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 225.3/307.7 kB 509.6 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 245.8/307.7 kB 502.2 kB/s eta 0:00:01\n",
      "   -------------------------------------  307.2/307.7 kB 575.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 307.7/307.7 kB 560.2 kB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8734c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sales_scaled  sell_price_scaled  month_scaled  year_scaled\n",
      "0      0.002826           0.018544      0.000000          0.0\n",
      "1      0.002826           0.018544      0.090909          0.0\n",
      "2      0.001695           0.018544      0.090909          0.0\n",
      "3      0.002826           0.018544      0.090909          0.0\n",
      "4      0.003673           0.018544      0.090909          0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sort data by item, store, and time to keep order\n",
    "weekly_data = weekly_data.sort_values(['item_id_cat', 'store_id_cat', 'wm_yr_wk'])\n",
    "\n",
    "# Select numeric features to scale\n",
    "numeric_features = ['sales', 'sell_price', 'month', 'year']\n",
    "\n",
    "scalers = {}\n",
    "for feature in numeric_features:\n",
    "    scaler = MinMaxScaler()\n",
    "    weekly_data[feature + '_scaled'] = scaler.fit_transform(weekly_data[[feature]])\n",
    "    scalers[feature] = scaler  # Save scaler for inverse transform later\n",
    "\n",
    "print(weekly_data[[f + '_scaled' for f in numeric_features]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd8f311",
   "metadata": {},
   "source": [
    "### Create Supervised Learning Sequences (Sliding Windows)\n",
    "We will create sequences of length n_steps (e.g., 10 weeks) of features to predict the sales of the next week.\n",
    "\n",
    "Important: Since we have multiple time series (item-store pairs), we create sequences per item-store and then combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da4a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\wwwsu\\desktop\\all folders\\logistics_demand\\venv\\lib\\site-packages (2.2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3dc43f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     y = np.array(y)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X_numeric, X_item, X_store, y\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m X_numeric, X_item, X_store, y = \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweekly_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumeric input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_numeric.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mItem input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_item.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcreate_sequences\u001b[39m\u001b[34m(df, n_steps)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Create sequences\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(group) - n_steps):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mX_numeric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     X_item.append(item_array[i+n_steps])   \u001b[38;5;66;03m# categorical for prediction time\u001b[39;00m\n\u001b[32m     25\u001b[39m     X_store.append(store_array[i+n_steps])\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_steps = 10  # number of past weeks to use as input\n",
    "\n",
    "def create_sequences(df, n_steps):\n",
    "    X_numeric, X_item, X_store, y = [], [], [], []\n",
    "    \n",
    "    # Group by item-store\n",
    "    grouped = df.groupby(['item_id_cat', 'store_id_cat'])\n",
    "    \n",
    "    for (item_cat, store_cat), group in grouped:\n",
    "        group = group.sort_values('wm_yr_wk')\n",
    "        \n",
    "        # Extract scaled numeric features as numpy array\n",
    "        features = group[[f + '_scaled' for f in numeric_features]].values\n",
    "        \n",
    "        # Extract categorical codes repeated for each time step\n",
    "        item_array = np.full((len(group),), item_cat)\n",
    "        store_array = np.full((len(group),), store_cat)\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(group) - n_steps):\n",
    "            X_numeric.append(features[i:i+n_steps])\n",
    "            X_item.append(item_array[i+n_steps])   # categorical for prediction time\n",
    "            X_store.append(store_array[i+n_steps])\n",
    "            y.append(features[i+n_steps][0])       # scaled sales at next time step\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_numeric = np.array(X_numeric)\n",
    "    X_item = np.array(X_item)\n",
    "    X_store = np.array(X_store)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X_numeric, X_item, X_store, y\n",
    "\n",
    "X_numeric, X_item, X_store, y = create_sequences(weekly_data, n_steps)\n",
    "\n",
    "print(f\"Numeric input shape: {X_numeric.shape}\")\n",
    "print(f\"Item input shape: {X_item.shape}\")\n",
    "print(f\"Store input shape: {X_store.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c0d88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
